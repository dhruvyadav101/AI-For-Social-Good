{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\death\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\death\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\death\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\death\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download as nltk_download\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import words as nltk_words\n",
    "\n",
    "# Download necessary resources\n",
    "nltk_download('stopwords')\n",
    "nltk_download('punkt')\n",
    "nltk_download('wordnet')\n",
    "nltk_download('words')  # To use a dictionary-based filtering\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\death\\\\Desktop\\\\minor_project\\\\datasets\\\\combined_dataset_before_pre_processing\\\\combined_data_before_preprocessing.csv')\n",
    "\n",
    "# Dictionary of valid English words (lowercase for comparison)\n",
    "valid_words = set(w.lower() for w in nltk_words.words())\n",
    "\n",
    "# Function to preprocess tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # Replace mentions (@user)\n",
    "    tweet = re.sub(r'@[^\\s]+', 'USER', tweet)\n",
    "    # Remove continuously repeated symbols or characters\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1', tweet)  # AAAB -> AA\n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "\n",
    "# Apply preprocessing to tweets\n",
    "df['clean_tweet'] = df['tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# Tokenization, Lemmatization, and Stopwords removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Remove stopwords, single characters, and gibberish-like words\n",
    "    tokens = [token for token in tokens if len(token) > 1 and token not in stop_words and (token in valid_words or re.match(r'^[a-z]+$', token))]\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization and lemmatization\n",
    "df['tokenized_tweet'] = df['clean_tweet'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Convert tokenized tweets back to strings\n",
    "df['clean_tweet_processed'] = df['tokenized_tweet'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Keep only the 'class', 'tweet', and 'clean_tweet_processed' columns\n",
    "df = df[['class', 'tweet', 'clean_tweet_processed']]\n",
    "\n",
    "# Save preprocessed data\n",
    "df.to_csv('preprocessed_data_multiclass.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved to preprocessed_data_binary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = 'preprocessed_data_multiclass.csv'  # Change to your CSV file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Update the 'class' column: Keep 'CONTROL' as is, change others to 'DIAGNOSED'\n",
    "df['class'] = df['class'].apply(lambda x: 'CONTROL' if x.upper() == 'CONTROL' else 'DIAGNOSED')\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_csv_path = 'preprocessed_data_binary.csv'  # Output CSV file path\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV file saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing rows with null values: 5170493\n",
      "Number of rows after removing rows with null values: 5163974\n",
      "First few rows of the cleaned DataFrame:\n",
      "  class                                              tweet  \\\n",
      "0  ADHD  \"@USER AAABDVSGJS NOO you're too kind ðŸ˜­ðŸ¥º more ...   \n",
      "1  ADHD                    \"wow! ðŸ¤© two years here HTTPURL\"   \n",
      "2  ADHD  \"nothing can compare to RK900 manboobies thoug...   \n",
      "3  ADHD  \"too bad his titties weren't bara sized huge a...   \n",
      "4  ADHD  \"ended up paying more than what was initially ...   \n",
      "\n",
      "                               clean_tweet_processed  \n",
      "0  user abdvsgjs kind like plagued everyone gdjsh...  \n",
      "1                                wow two year htpurl  \n",
      "2             nothing compare manbobies though setle  \n",
      "3  bad tities bara sized huge rather wa flat cant...  \n",
      "4  ended paying wa initialy agred hairdreser guy ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file_path = 'preprocessed_data_multiclass.csv'  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the number of rows before removing missing data\n",
    "print(\"Number of rows before removing rows with null values:\", len(df))\n",
    "\n",
    "# Drop rows with any missing/null values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Display the number of rows after removing missing data\n",
    "print(\"Number of rows after removing rows with null values:\", len(df_cleaned))\n",
    "\n",
    "# Optionally, you can save the cleaned DataFrame to a new CSV file\n",
    "output_csv_path = 'C:\\\\Users\\\\death\\\\Desktop\\\\minor_project\\\\datasets\\\\main_dataset\\\\preprocessed_data_multiclass.csv'  # Change this to your desired output path\n",
    "df_cleaned.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"First few rows of the cleaned DataFrame:\")\n",
    "print(df_cleaned.head())  # Display the first few rows of the cleaned DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing rows with null values: 5170493\n",
      "Number of rows after removing rows with null values: 5163974\n",
      "First few rows of the cleaned DataFrame:\n",
      "       class                                              tweet  \\\n",
      "0  DIAGNOSED  \"@USER AAABDVSGJS NOO you're too kind ðŸ˜­ðŸ¥º more ...   \n",
      "1  DIAGNOSED                    \"wow! ðŸ¤© two years here HTTPURL\"   \n",
      "2  DIAGNOSED  \"nothing can compare to RK900 manboobies thoug...   \n",
      "3  DIAGNOSED  \"too bad his titties weren't bara sized huge a...   \n",
      "4  DIAGNOSED  \"ended up paying more than what was initially ...   \n",
      "\n",
      "                               clean_tweet_processed  \n",
      "0  user abdvsgjs kind like plagued everyone gdjsh...  \n",
      "1                                wow two year htpurl  \n",
      "2             nothing compare manbobies though setle  \n",
      "3  bad tities bara sized huge rather wa flat cant...  \n",
      "4  ended paying wa initialy agred hairdreser guy ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file_path = 'preprocessed_data_binary.csv'  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the number of rows before removing missing data\n",
    "print(\"Number of rows before removing rows with null values:\", len(df))\n",
    "\n",
    "# Drop rows with any missing/null values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Display the number of rows after removing missing data\n",
    "print(\"Number of rows after removing rows with null values:\", len(df_cleaned))\n",
    "\n",
    "# Optionally, you can save the cleaned DataFrame to a new CSV file\n",
    "output_csv_path = 'C:\\\\Users\\\\death\\\\Desktop\\\\minor_project\\\\datasets\\\\main_dataset\\\\preprocessed_data_binary.csv'  # Change this to your desired output path\n",
    "df_cleaned.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"First few rows of the cleaned DataFrame:\")\n",
    "print(df_cleaned.head())  # Display the first few rows of the cleaned DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
